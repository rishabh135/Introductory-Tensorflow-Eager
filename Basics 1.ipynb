{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a158877b25ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5488\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5490\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5492\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset, Iterator\n",
    "\n",
    "import glob\n",
    "import os,time\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor is a multi-dimensional array. Similar to NumPy ndarray objects, Tensor objects have a data type and a shape. Additionally, Tensors can reside in accelerator (like GPU) memory. TensorFlow offers a rich library of operations (tf.add, tf.matmul, tf.linalg.inv etc.) that consume and produce Tensors. These operations automatically convert native Python types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())  \n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(25, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(b'aGVsbG8gd29ybGQ', shape=(), dtype=string)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.add(1, 2))\n",
    "print(tf.add([1, 2], [3, 4]))\n",
    "print(tf.square(5))\n",
    "print(tf.reduce_sum([1, 2, 3]))\n",
    "print(tf.encode_base64(\"hello world\"))\n",
    "\n",
    "# Operator overloading is also supported\n",
    "print(tf.square(2) + tf.square(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor has both dtype and sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.matmul([[1]], [[2, 3]])\n",
    "print(x.shape)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion between tensorflow and numpy dtypes\n",
    "TensorFlow operations automatically convert NumPy ndarrays to Tensors.\n",
    "NumPy operations automatically convert Tensors to NumPy ndarrays.\n",
    "Tensors can be explicitly converted to NumPy ndarrays by invoking the .numpy() method on them. These conversions are typically cheap as the array and Tensor share the underlying memory representation if possible. However, sharing the underlying representation isn't always possible since the Tensor may be hosted in GPU memory while NumPy arrays are always backed by host memory, and the conversion will thus involve a copy from GPU to host memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow operations convert numpy arrays to Tensors automatically\n",
      "tf.Tensor(\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
      "And NumPy operations convert Tensors to numpy arrays automatically\n",
      "[[43. 43. 43.]\n",
      " [43. 43. 43.]\n",
      " [43. 43. 43.]]\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
    "tensor = tf.multiply(ndarray, 42)\n",
    "print(tensor)\n",
    "\n",
    "\n",
    "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
    "print(np.add(tensor, 1))\n",
    "\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a GPU available: \n",
      "True\n",
      "Is the Tensor on GPU #0:  \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = tf.random_uniform([3, 3])\n",
    "\n",
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "print(\"Is the Tensor on GPU #0:  \"),\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=90, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.02160025, 0.8575293 , 0.59637415],\n",
       "       [0.6386746 , 0.35419858, 0.07505405],\n",
       "       [0.71760154, 0.5081749 , 0.49256754]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorflow to use the data (images, text, sound, etc) we create tf.data.dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a CSV file\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "            Line 1 \n",
    "            Line 2\n",
    "            Line 3\n",
    "              \"\"\")\n",
    "\n",
    "ds_file = tf.data.TextLineDataset(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images we do something similar, here is an example where we load CelebA dataset and create an appropriate dataset for the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CelebA from \"../gan_assignment/data_for_gan_assignment/celebA/\"\n",
      "Number of image files loaded: 121072 \n",
      "Images loaded have width 64 and heigth 64\n"
     ]
    }
   ],
   "source": [
    "celeba_dir = \"../gan_assignment/data_for_gan_assignment/celebA/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Loading CelebA from \"%s\"' % celeba_dir)\n",
    "glob_pattern = os.path.join(celeba_dir,  '*.jpg')\n",
    "\n",
    "image_filenames = sorted(glob.glob(glob_pattern))\n",
    "print(\"Number of image files loaded: {} \" .format(len(image_filenames)))\n",
    "\n",
    "\n",
    "\n",
    "im = PIL.Image.open(image_filenames[100])\n",
    "width, height = im.size\n",
    "\n",
    "print(\"Images loaded have width {} and heigth {}\".format(width,height)) \n",
    "\n",
    "def parse_function(filename):\n",
    "    image_string = tf.read_file(filename)\n",
    "\n",
    "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    # image = tf.image.resize_images(image, [resize_size, resize_size])\n",
    "    return image\n",
    "\n",
    "\n",
    "def train_preprocess(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "\n",
    "    # Make sure the image is still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Setting up the data processing pipeline\n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices(image_filenames)\n",
    "dataset = dataset.shuffle(len(image_filenames))\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=4)\n",
    "#dataset = dataset.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Gradient tape and its examples\n",
    "\n",
    "Record operations for automatic differentiation.\n",
    "\n",
    "Operations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".\n",
    "\n",
    "Trainable variables (created by tf.Variable or tf.get_variable, trainable=True is default in both cases) are automatically watched. Tensors can be manually watched by invoking the watch method on this context manager.\n",
    "\n",
    "For example, consider the function y = x * x. The gradient at x = 3.0 can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf18]",
   "language": "python",
   "name": "conda-env-tf18-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
